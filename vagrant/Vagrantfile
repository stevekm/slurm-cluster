# -*- mode: ruby -*-
# vi: set ft=ruby :

# Define the list of machines in the cluster
slurm_cluster = {
    :headnode => {
        :hostname => "headnode",
        :ipaddress => "10.10.10.3"
    },
    :node00 => {
        :hostname => "node00",
        :ipaddress => "10.10.10.4"
    },
    :node01 => {
        :hostname => "node01",
        :ipaddress => "10.10.10.5"
    }
}

# Provisioning inline script for all nodes
$script = <<SCRIPT
set -x

# install packages needed for SLURM
yum upgrade
yum install -y epel-release
yum install -y bind-utils \
expect \
gcc \
git \
gtk2-devel \
hdf5-devel \
htop \
hwloc \
hwloc-devel \
hwloc-gui \
libibmad \
libibumad \
libcurl \
libssh2-devel \
lua \
lua-devel \
man \
make \
man2html \
mariadb \
mariadb-devel \
mariadb-server \
munge \
munge-devel \
munge-libs \
net-tools \
ncurses-devel \
nfs-utils \
numactl \
numactl-devel \
openssh-server \
openssl-devel \
pam-devel \
perl-ExtUtils-MakeMaker \
python-pip \
readline-devel \
rpm-build \
rrdtool-devel \
wget \
pdsh

# download and compile SLURM source code
wget https://download.schedmd.com/slurm/slurm-18.08.8.tar.bz2 && \
tar -jxvf slurm-18.08.8.tar.bz2 && \
rm -f slurm-18.08.8.tar.bz2 && \
mv slurm-18.08.8 /root/

cd /root/slurm-18.08.8 && \
./configure && \
make -j 2 && \
make install -j 2

# import SLURM config files from Vagrant host
cp /vagrant/slurm.conf /usr/local/etc/slurm.conf
cp /vagrant/slurmdbd.conf /usr/local/etc/slurmdbd.conf

# import SLURM service files from source
cp /root/slurm-18.08.8/etc/slurmdbd.service /usr/lib/systemd/system
cp /root/slurm-18.08.8/etc/slurmd.service /usr/lib/systemd/system
cp /root/slurm-18.08.8/etc/slurmctld.service /usr/lib/systemd/system

systemctl enable slurmd.service
systemctl enable slurmdbd.service
systemctl enable slurmctld.service
systemctl enable munge
systemctl enable mariadb

# reset the system daemon configs and start required services
systemctl daemon-reload

# import munge key file
cp /vagrant/munge.key /etc/munge/munge.key
chown munge:munge /etc/munge/munge.key
chmod 600 /etc/munge/munge.key

# setup 'slurm' user and required directories
useradd -m slurm
mkdir /var/spool/slurmd
chown slurm:slurm /var/spool/slurmd
mkdir /var/spool/slurmctld
chown slurm:slurm /var/spool/slurmctld
mkdir /var/log/slurm/
chown slurm:slurm /var/log/slurm/

# set up pdsh for easier manual provisioning later (optional)
mkdir /etc/PDSH
printf 'node00\nnode01\n' > /etc/PDSH/hosts
echo 'WCOLL=/etc/PDSH/hosts' >> /etc/profile

# update PATH for SLURM binaries
echo 'PATH=/usr/local/bin/:$PATH' >> /etc/profile

# update hosts file for Vagrant cluster IP addresses
echo "10.10.10.3    headnode" >> /etc/hosts
echo "10.10.10.4    node00" >> /etc/hosts
echo "10.10.10.5    node01" >> /etc/hosts
SCRIPT

# extra provisioning script for just the head node
$headnode_script = <<SCRIPT
set -xe

systemctl start munge
sleep 5
systemctl status munge

systemctl start mariadb
sleep 5
systemctl status mariadb

# need to create the slurm database user
mysql -e "create user 'slurm'@'localhost' identified by 'password'"
mysql -e "grant all on slurm_acct_db.* TO 'slurm'@'localhost'"

# start the SLURM database service
systemctl start slurmdbd
sleep 5
systemctl status slurmdbd

# wait and then check if the database is available
sleep 10
netstat -antp | grep LISTEN

# set up the accounting for the cluster
sacctmgr -i create cluster cluster
sacctmgr -i create account cluster_account
sacctmgr -i create user steve account=cluster_account

# finally start the SLURM controller daemon
systemctl start slurmctld
sleep 5
systemctl status slurmctld

SCRIPT

# provisioning script for compute nodes
$node_script = <<SCRIPT
set -xe

systemctl start munge
sleep 5
systemctl status munge

systemctl start slurmd
sleep 5
systemctl status slurmd

SCRIPT

$headnode_script2 = <<SCRIPT
set -xe

sinfo

scontrol update nodename=node00 state=resume
sleep 5

scontrol update nodename=node01 state=resume
sleep 5

sinfo

SCRIPT

# troubleshooting;
# [root@headnode ~]# cat /var/log/messages
# [root@headnode ~]# cat /var/log/slurmctld.log

Vagrant.configure("2") do |global_config|
    # initial setup of each VM in the cluster
    slurm_cluster.each_pair do |name, options|
        global_config.vm.define name do |config|
            # VM configurations
            config.vm.box = "centos/7" # "generic/rhel7"
            config.vm.hostname = "#{name}"
            config.vm.network :private_network, ip: options[:ipaddress]

            # VM specifications
            config.vm.provider :virtualbox do |v|
                v.cpus = 2
                v.memory = 1024
            end

            # VM provisioning
            config.vm.provision :shell, :inline => $script
        end
    end

    # extra provisioning steps for head node
    global_config.vm.define "headnode", primary: true do |headnode|
        headnode.vm.provision :shell, :inline => $headnode_script
    end

    global_config.vm.define "node00", primary: true do |node00|
        node00.vm.provision :shell, :inline => $node_script
    end

    global_config.vm.define "node01", primary: true do |node00|
        node00.vm.provision :shell, :inline => $node_script
    end

end
